# 决策树

决策树是一种监督学习，可以做分类问题，也可以做回归问题

## 基本流程

### 组成

一般，一棵决策树包含一个根节点，若干个内部节点（非叶子节点）和若干个叶子节点。

1. 根节点：第一个选择点
2. 内部节点（非叶子节点）：中间决策过程
3. 叶子节点：最终的决策结果

### 训练流程

决策树的构建是一个递归过程，递归结束条件有如下：  
1. 当前结点包含的样本全属于同一类别

1. 当前属性集为空，或所有样本在所有属性上取值相同

2. 当前结点包含的样本集为空

**注意**：

* 在2），我们把当前结点标记为叶结点，并将其类别设定为该结点所含样本最多的类别
* 3），也标记为叶结点，但类别设定为其父结点所含样本最多的类别

## 决策树划分选择

决策树算法的核心是如何选择最优的划分特征，我们希望随着划分的进行，我们经过每次划分后的分支节点所包含的样本尽可能的属于同一类别，也就是节点中所包含的样本纯度越来越高。

### 信息熵

信息熵表示的是随机变量不确定性的度量，熵越大，不确定性越强，也就是说纯度越低；  
熵越小，不确定性越弱，纯度越高。

$$假定当前样本集合D中第k类样本所占比例为p_k(k=1,2,3...|y|)$$,则D的信息熵定义为：


$$
Ent(D) = -\sum_{k=1}^{|y|}p_k\log_{2}{p_k}
$$


Ent\(D\)的值越小，则D的纯度越高。

### 信息增益

假定离散属性a有V个可能的取值$${a^1, a^2,a^3,...a^V}$$,若使用a来对样本集D进行划分，则会产生V个分支结点，其中第v个分支结点包含了D中所有在属性a上取值为$a^v$的样本，记为$$D^v$$.


$$
Gain(D,a) = Ent(D)-\sum_{v=1}^{V}\frac{|D^v|}{D}Ent(D^v)
$$


一般来说,信息增益越大，使用a属性来进行划分所获得的“纯度提升”越大。

西瓜数据2.0数据

|  | 色泽 | 根蒂 | 敲声 | 纹理 | 脐部 | 触感 | 密度 | 含糖率 | 好瓜 |
| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |
| 1 | 青绿 | 蜷缩 | 浊响 | 清晰 | 凹陷 | 硬滑 | 0.697 | 0.46 | 1 |
| 2 | 乌黑 | 蜷缩 | 沉闷 | 清晰 | 凹陷 | 硬滑 | 0.774 | 0.376 | 1 |
| 3 | 乌黑 | 蜷缩 | 浊响 | 清晰 | 凹陷 | 硬滑 | 0.634 | 0.264 | 1 |
| 4 | 青绿 | 蜷缩 | 沉闷 | 清晰 | 凹陷 | 硬滑 | 0.608 | 0.318 | 1 |
| 5 | 浅白 | 蜷缩 | 浊响 | 清晰 | 凹陷 | 硬滑 | 0.556 | 0.215 | 1 |
| 6 | 青绿 | 稍蜷 | 浊响 | 清晰 | 稍凹 | 软粘 | 0.403 | 0.237 | 1 |
| 7 | 乌黑 | 稍蜷 | 浊响 | 稍糊 | 稍凹 | 软粘 | 0.481 | 0.149 | 1 |
| 8 | 乌黑 | 稍蜷 | 浊响 | 清晰 | 稍凹 | 硬滑 | 0.437 | 0.211 | 1 |
| 9 | 乌黑 | 稍蜷 | 沉闷 | 稍糊 | 稍凹 | 硬滑 | 0.666 | 0.091 | 0 |
| 10 | 青绿 | 硬挺 | 清脆 | 清晰 | 平坦 | 软粘 | 0.243 | 0.267 | 0 |
| 11 | 浅白 | 硬挺 | 清脆 | 模糊 | 平坦 | 硬滑 | 0.245 | 0.057 | 0 |
| 12 | 浅白 | 蜷缩 | 浊响 | 模糊 | 平坦 | 软粘 | 0.343 | 0.099 | 0 |
| 13 | 青绿 | 稍蜷 | 浊响 | 稍糊 | 凹陷 | 硬滑 | 0.639 | 0.161 | 0 |
| 14 | 浅白 | 稍蜷 | 沉闷 | 稍糊 | 凹陷 | 硬滑 | 0.657 | 0.198 | 0 |
| 15 | 乌黑 | 稍蜷 | 浊响 | 清晰 | 稍凹 | 软粘 | 0.36 | 0.37 | 0 |
| 16 | 浅白 | 蜷缩 | 浊响 | 模糊 | 平坦 | 硬滑 | 0.593 | 0.042 | 0 |
| 17 | 青绿 | 蜷缩 | 沉闷 | 稍糊 | 稍凹 | 硬滑 | 0.719 | 0.103 | 0 |



