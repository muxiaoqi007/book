# 决策树

决策树是一种监督学习，可以做分类问题，也可以做回归问题

## 基本流程

### 组成

一般，一棵决策树包含一个根节点，若干个内部节点（非叶子节点）和若干个叶子节点。

1. 根节点：第一个选择点
2. 内部节点（非叶子节点）：中间决策过程
3. 叶子节点：最终的决策结果

### 训练流程

决策树的构建是一个递归过程，递归结束条件有如下：  
1. 当前结点包含的样本全属于同一类别

1. 当前属性集为空，或所有样本在所有属性上取值相同

2. 当前结点包含的样本集为空

**注意**：

* 在2），我们把当前结点标记为叶结点，并将其类别设定为该结点所含样本最多的类别
* 3），也标记为叶结点，但类别设定为其父结点所含样本最多的类别

## 决策树划分选择

决策树算法的核心是如何选择最优的划分特征，我们希望随着划分的进行，我们经过每次划分后的分支节点所包含的样本尽可能的属于同一类别，也就是节点中所包含的样本纯度越来越高。

### 信息熵

信息熵表示的是随机变量不确定性的度量，熵越大，不确定性越强，也就是说纯度越低；  
熵越小，不确定性越弱，纯度越高。

$$假定当前样本集合D中第k类样本所占比例为p_k(k=1,2,3...|y|)$$,则D的信息熵定义为：


$$
Ent(D) = -\sum_{k=1}^{|y|}p_k\log_{2}{p_k}
$$


Ent\(D\)的值越小，则D的纯度越高。

### 信息增益

$$假定离散属性a有V个可能的取值{a^1, a^2,a^3,...a^V},若使用a来对样本集D进行划分$$，则会$$产生V个分支结点，其中第v个分支结点包含了D中所有在属性a上取值为a^v的样本，记D^v$$.


$$
Gain(D,a) = Ent(D)-\sum_{v=1}^{V}\frac{|D^v|}{D}Ent(D^v)
$$


一般来说,信息增益越大，使用a属性来进行划分所获得的“纯度提升”越大。

西瓜数据2.0数据

|  | 色泽 | 根蒂 | 敲声 | 纹理 | 脐部 | 触感 | 密度 | 含糖率 | 好瓜 |
| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |
| 1 | 青绿 | 蜷缩 | 浊响 | 清晰 | 凹陷 | 硬滑 | 0.697 | 0.46 | 1 |
| 2 | 乌黑 | 蜷缩 | 沉闷 | 清晰 | 凹陷 | 硬滑 | 0.774 | 0.376 | 1 |
| 3 | 乌黑 | 蜷缩 | 浊响 | 清晰 | 凹陷 | 硬滑 | 0.634 | 0.264 | 1 |
| 4 | 青绿 | 蜷缩 | 沉闷 | 清晰 | 凹陷 | 硬滑 | 0.608 | 0.318 | 1 |
| 5 | 浅白 | 蜷缩 | 浊响 | 清晰 | 凹陷 | 硬滑 | 0.556 | 0.215 | 1 |
| 6 | 青绿 | 稍蜷 | 浊响 | 清晰 | 稍凹 | 软粘 | 0.403 | 0.237 | 1 |
| 7 | 乌黑 | 稍蜷 | 浊响 | 稍糊 | 稍凹 | 软粘 | 0.481 | 0.149 | 1 |
| 8 | 乌黑 | 稍蜷 | 浊响 | 清晰 | 稍凹 | 硬滑 | 0.437 | 0.211 | 1 |
| 9 | 乌黑 | 稍蜷 | 沉闷 | 稍糊 | 稍凹 | 硬滑 | 0.666 | 0.091 | 0 |
| 10 | 青绿 | 硬挺 | 清脆 | 清晰 | 平坦 | 软粘 | 0.243 | 0.267 | 0 |
| 11 | 浅白 | 硬挺 | 清脆 | 模糊 | 平坦 | 硬滑 | 0.245 | 0.057 | 0 |
| 12 | 浅白 | 蜷缩 | 浊响 | 模糊 | 平坦 | 软粘 | 0.343 | 0.099 | 0 |
| 13 | 青绿 | 稍蜷 | 浊响 | 稍糊 | 凹陷 | 硬滑 | 0.639 | 0.161 | 0 |
| 14 | 浅白 | 稍蜷 | 沉闷 | 稍糊 | 凹陷 | 硬滑 | 0.657 | 0.198 | 0 |
| 15 | 乌黑 | 稍蜷 | 浊响 | 清晰 | 稍凹 | 软粘 | 0.36 | 0.37 | 0 |
| 16 | 浅白 | 蜷缩 | 浊响 | 模糊 | 平坦 | 硬滑 | 0.593 | 0.042 | 0 |
| 17 | 青绿 | 蜷缩 | 沉闷 | 稍糊 | 稍凹 | 硬滑 | 0.719 | 0.103 | 0 |

## 练习

计算当前属性集合{色泽，根蒂，敲声，纹理，脐部，触感}中每个属性的信息增益，以属性**色泽**为例，取值为{青绿，乌黑，浅白}，则：

$$D^1(色泽=青绿):{1,4,6,10,13,17}，正例：3 反例：3$$

$$D^2(色泽=乌黑):{2，3，7，8，9，15}，正例：4反例：2$$

$$D^3(色泽=浅白):{5，11，12，14，16}，正例：1 反例：5​$$

则：
$$
Ent(D)=-\sum_{k=1}^{2}p_2\log_2p_k=-(\frac{8}{17}\log_2\frac{8}{17}+\frac{9}{17}\log_2\frac{9}{17})=0.998,\\
Ent(D^1) = -(\frac{3}{6}\log_2\frac{3}{6}+\frac{3}{6}\log_2\frac{3}{6})=1.000,\\
Ent(D^2)=-(\frac{4}{6}\log_2\frac{4}{6}+\frac{2}{6}\log_2\frac{2}{6})=0.918 \\
Ent(D^3) = -(\frac{1}{5}\log_2\frac{1}{5}+\frac{4}{5}\log_2\frac{4}{5})=0.722 \\
$$

$$
\begin{align}
Gain(D,色泽) & = Ent(D)-\sum_{v=1}^{3}\frac{|D^v|}{|D|}Ent(D^v) \\
& = 0.998-(\frac{6}{17}*1.000+\frac{6}{17}*0.918+\frac{5}{17}*0.722) \\
& = 0.109
\end{align}
$$

同理可计算出其他属性的。

## 增益率

信息增益准则对可取数目较多的属性有所偏好，为减少这种偏好可能带来的不利影响，C4.5算法使用增益率来选择最优划分属性，定义如下：
$$
Gain\_ratio(D,a) = \frac{Gain(D,a)}{IV(a)},
\\其中,IV(a)=-\sum_{v=1}^{V}\frac{|D^v|}{|D|}\log_2\frac{|D^v|}{|D|}
$$
通常属性a的可能取值数目越多（即V越大），则IV(a)的值越大，增益率准则对可取值数目较少的属性有所偏好，所以，C4.5先从候选划分属性中找出信息增益高于平均水平的属性，再从中选择增益率最高的。



## 基尼指数

CART决策树使用基尼指数，定义如下：
$$
\begin{align}
Gini(D)&=\sum_{k=1}^{|y|}\sum_{k'\neq{k}}p_kp_{k'}\\
&=1-\sum_{k=1}^{|y|}p_k^2
\end{align}
$$

Gini(D)反映了从数据集D中随机抽取两个样本，其类别标记不一样的概率，**所以Gini(D)越小，D的纯度越高**

属性a的基尼指数定义：
$$
Gini\_index(D,a)=\sum_{v=1}^{V}\frac{|D^v|}{|D|}Gini(D^v)
$$
