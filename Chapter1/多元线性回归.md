## 多元线性回归

目标：使$$\sum_{i=1}^{m}(y_i-\hat{y_i})^2$$,尽可能的小
$$
\hat{y}^{i}=w_0+w_1{x_1}^{i}+w_2{x_2}^{i}+...+w_n{x_n}^{i}
$$

即找到$${w_0,w_1,w_2,...,w_n}$$,使得目标值 尽可能地小

设
$$
W = \mathbf{(w_0,w_1,w_2,...,w_n)}^\mathrm{T}
$$

$$
\hat{y}^{i}=w_0{x_0}^{i}+w_1{x_1}^{i}+w_2{x_2}^{i}+...+w_n{x_n}^{i},{x_0}^{i}\equiv1
$$

$$
X^{i} = \mathbf{(X_0^{i},X_1^{i},X_2^{i},...,X_n^{i})}^\mathrm{T}
$$

$$
\hat{y}^{i}=X^{i}W
$$

推广：
$$
X_b=\begin{pmatrix}
        1 & X_1^{(1)} & X_2^{(1)} & \cdots & X_n^{(1)} \\
        1 &  X_1^{(2)} & X_2^{(2)} & \cdots & X_n^{(2)} \\
        \vdots & \vdots & \vdots & \ddots & \vdots \\
        1 &  X_1^{(m)} & X_2^{(m)} & \cdots & X_n^{(m)} \\
        \end{pmatrix}
$$
$$
W =\begin{pmatrix}
        W_0\\W_1\\W_2\\\vdots \\W_n
        \end{pmatrix}
$$
$$
\hat{y}=X_bW
$$

使得目标值尽量小，即：
$$
J(W)=\sum_{i=1}^{m}(y_i-\hat{y_i})^2=\mathbf{(Y-X_bW)}^\mathrm{T}(Y-X_bW)
$$
这里还需要用到向量内积对向量求导的知识，对同维度的两个向量U,V,二者向量内积对任意维列向量X求导
$$
\frac{d(\mathbf{U}^\mathrm{T}V)}{dx}=\frac{d(\mathbf{U}^\mathrm{T})}{dx}V+\frac{d(\mathbf{X}^\mathrm{T})}{dx}U
$$
同理
$$
\frac{d(\mathbf{X}^\mathrm{T}X)}{dx}=\frac{d(\mathbf{X}^\mathrm{T})}{dx}X+\frac{d(\mathbf{X}^\mathrm{T})}{dx}X=2IX=2X
$$

$$
\frac{d(\mathbf{X}^\mathrm{T}X)}{dy}=\frac{d(\mathbf{X}^\mathrm{T})}{dy}X+\frac{d(\mathbf{X}^\mathrm{T})}{dy}X=2\frac{d(X^T)}{dy}X
$$

则：
$$
\frac{\partial J(W)}{\partial W}=\frac{2\partial\mathbf{(Y-X_bW)}^\mathrm{T}}{\partial W}（Y-X_bW)
$$

$$
=2\frac{\partial \mathbf{Y}^\mathrm{T}}{\partial W}(Y-X_bW)-2\frac{\partial \mathbf{(X_bW)}^\mathrm{T}}{\partial W}(Y-X_bW)
$$

$$
=0 - 2\mathbf{X_b}^{T}(Y-X_bW)
$$


在处理较为复杂的数据的回归问题时，普通的线性回归算法通常会出现预测精度不够，如果模型中的特征之间有相关关系，就会增加模型的复杂程度，并且对整个模型的解释能力并没有提高，这时，就需要对数据中的特征进行选择。
