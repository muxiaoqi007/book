---
title: 线性回归
date: 2019-03-12 00:32:26
tags: 机器学习
---
线性回归本质上是一个函数估计的问题，即找出自变量与因变量的关系。
 - 因变量是连续的，回归
 - 因变量是离散的，分类
  回归分析是一个有监督的学习
  <!--more-->
## 简单线性回归
假设某个体x有d个特征，即x=(x^1,x^2,...,x^d)，x^i是第i个特征，线性模型(linear model)试图通过特征的线性组合得到预测值，即
$$
f(x)=w^{T}x+b=w_{1}x^{1}+w_{2}x^{2}+...+w_{d}x^{n}+b
$$
<!--more-->
 其中当w_{i}是第个特征的权重，既能调节特征的量纲，也能显示该特征对预测值的重要程度；是第i个特征的权重，既能调节特征的量纲，也能显示该特征对预测值的重要程度
$$
w^{T}=（w_{1}，w_{2}，...，w_{d}）
$$

$$
x^{T}=（x_{1}，x_{2}，...，x_{d}）
$$

b代表预测值中非代表预测值中非x所能影响的那部分；当所能影响的那部分；当d=1时，便是最简单的线性模型时，便是最简单的线性模型
$$
f(x)=wx+b
$$


线性回归试图学习f(x_i)=wx_i+b，使得f(x_i)≈y_i
$$
g(w,b)=\sum_{i=1}^{n}(f(x_{i})-y_{i})^{2}=\sum_{i=1}^{n}(wx_{i}+b-y_{i})^{2}
$$
让g(w,b)取得最小值。因此我们可以用偏导数求解： 
$$
\frac{\partial g(w,b)}{\partial w}=0\\
\frac{\partial g(w,b)}{\partial b}=0
$$


&emsp;&emsp;解出：

$$
w=\frac{\sum_{i=1}^{n}y_{i}(x_{i}-\bar{x})}{\sum_{i=1}^{n}x_{i}^2-n\bar{x}^{2}}\\
b=\bar{y}-w\bar{x}\\
\bar{x}=\frac{1}{n}\sum_{i=1}^{n}x_{i}\\
\bar{y}=\frac{1}{n}\sum_{i=1}^{n}y_{i}
$$
推导过程如下：

- 先求b:
  $$
  \frac{\partial g(w,b)}{\partial b}=\sum_{i=1}^{n}(y_{i}-wx_i-b)(-1)
  =\sum_{i=1}^{n}(y_{i}-wx_i-b)=0
  $$
  化简得：
  $$
  \sum_{i=1}^{n}y_i-w\sum_{i=1}^{n}x_i-\sum_{i=1}^{n}b=\sum_{i=1}^{n}y_i-w\sum_{i=1}^{n}x_i-mb=0
  $$
  即：
  $$
  mb=\sum_{i=1}^{n}y_i-w\sum_{i=1}^{n}x_i
  $$
  即：
  $$
  b=\bar{y}-w\bar{x}
  $$

  - 再求w
    $$
    \frac{\partial g(w,b)}{\partial b}=\sum_{i=1}^{n}2(y_{i}-wx_i-b)(-x_i)=\sum_{i=1}^{n}(y_{i}-wx_i-b)x_i=0
    $$
    代入b：
    $$
    \frac{\partial g(w,b)}{\partial b}=\sum_{i=1}^{n}(y_i-wx_i-\bar{y}+w\bar{x})x_i=0
    $$
    即：
    $$
    \sum_{i=1}^{n}(x_iy_i-wx_i^2-x_i\bar{y}+wx_i\bar{x})=0
    $$
    即：
    $$
    \sum_{i=1}^{n}(x_iy_i-x_i\bar{y})=\sum_{i=1}^{n}(wx_i^2-wx_i\bar{x})
    $$
    即：
    $$
    w=\frac{\sum_{i=1}^{n}(x_iy_i-x_i\bar{y})}{\sum_{i=1}^{n}(x_i^2-x_i\bar{x})}
    $$
    w的解这样看起来很复杂，可以进一步优化

    其中：
    $$
    \sum_{i=1}^{n}x_i\bar{y}=\bar{y}\sum_{i=1}^{n}x_i=n\bar{y}\bar{x}=\bar{x}\sum_{i=1}^{n}\bar{y}=\sum_{i=1}^{n}\bar{x}y_i=\sum_{i=1}^{n}\bar{x}\bar{y}
    $$





             所以：
$$
w=\frac{\sum_{i=1}^{n}(x_iy_i-x_i\bar{y}-\bar{x}y_i+\bar{x}\bar{y})}{\sum_{i=1}^{n}(x_i^2-x_i\bar{x}-\bar{x}x_i+\bar{x})}=\frac{\sum_{i=1}^{n}(x_i-\bar{x})(y_i-\bar{y})}{\sum_{i=1}^{n}(x_i-\bar{x})^2}
$$



## 实现简单线性回归

```python
import numpy as np
import matplotlib.pyplot as plt
x = np.array([1.,2.,3.,4.,5.])
y = np.array([2.,3.,4.,3.,5.])
x_mean = np.mean(x)
y_mean = np.mean(y)
n = d = 0.0
for x_i, y_i in zip(x, y):
	n += (x_i-x_mean)*(y_i-y_mean)
	d += (x_i-x_mean)**2
w = n/d
b = y_mean - w*x_mean
y_hat = a*x + b
```

### 线性回归的评价指标

均方误差 MSE：$\frac{1}{n}\sum_{i=1}^{n}(y_i-\hat{y_i})^2$

均方根误差 RMSE $\sqrt{\frac{1}{n}\sum_{i=1}^{n}(y_i-\hat{y_i})^2}$

平均绝对误差 MAE:  $\frac{1}{n}\sum_{i=1}^{n}|y_i-\hat{y_i}|$

R-squared : $1 - \frac{MSE(\hat{y}, y)}{Var(y)}$

## 多元线性回归

目标：使\sum_{i=1}^{m}(y_i-\hat{y_i})^2,尽可能的小
$$
\hat{y}^{i}=w_0+w_1{x_1}^{i}+w_2{x_2}^{i}+...+w_n{x_n}^{i}
$$

即找到${w_0,w_1,w_2,...,w_n}$,使得目标值 尽可能地小

设
$$
W = \mathbf{(w_0,w_1,w_2,...,w_n)}^\mathrm{T}
$$

$$
\hat{y}^{i}=w_0{x_0}^{i}+w_1{x_1}^{i}+w_2{x_2}^{i}+...+w_n{x_n}^{i},{x_0}^{i}\equiv1
$$

$$
X^{i} = \mathbf{(X_0^{i},X_1^{i},X_2^{i},...,X_n^{i})}^\mathrm{T}
$$

$$
\hat{y}^{i}=X^{i}W
$$

推广：
$$
X_b=\begin{pmatrix}
        1 & X_1^{(1)} & X_2^{(1)} & \cdots & X_n^{(1)} \\
        1 &  X_1^{(2)} & X_2^{(2)} & \cdots & X_n^{(2)} \\
        \vdots & \vdots & \vdots & \ddots & \vdots \\
        1 &  X_1^{(m)} & X_2^{(m)} & \cdots & X_n^{(m)} \\
        \end{pmatrix}
$$
$$
W =\begin{pmatrix}
        W_0\\W_1\\W_2\\\vdots \\W_n
        \end{pmatrix}
$$
$$
\hat{y}=X_bW
$$

使得目标值尽量小，即：
$$
J(W)=\sum_{i=1}^{m}(y_i-\hat{y_i})^2=\mathbf{(Y-X_bW)}^\mathrm{T}(Y-X_bW)
$$
这里还需要用到向量内积对向量求导的知识，对同维度的两个向量U,V,二者向量内积对任意维列向量X求导
$$
\frac{d(\mathbf{U}^\mathrm{T}V)}{dx}=\frac{d(\mathbf{U}^\mathrm{T})}{dx}V+\frac{d(\mathbf{X}^\mathrm{T})}{dx}U
$$
同理
$$
\frac{d(\mathbf{X}^\mathrm{T}X)}{dx}=\frac{d(\mathbf{X}^\mathrm{T})}{dx}X+\frac{d(\mathbf{X}^\mathrm{T})}{dx}X=2IX=2X
$$

$$
\frac{d(\mathbf{X}^\mathrm{T}X)}{dy}=\frac{d(\mathbf{X}^\mathrm{T})}{dy}X+\frac{d(\mathbf{X}^\mathrm{T})}{dy}X=2\frac{d(X^T)}{dy}X
$$

则：
$$
\frac{\partial J(W)}{\partial W}=\frac{2\partial\mathbf{(Y-X_bW)}^\mathrm{T}}{\partial W}（Y-X_bW)
$$

$$
=2\frac{\partial \mathbf{Y}^\mathrm{T}}{\partial W}(Y-X_bW)-2\frac{\partial \mathbf{(X_bW)}^\mathrm{T}}{\partial W}(Y-X_bW)
$$

$$
=0 - 2\mathbf{X_b}^{T}(Y-X_bW)
$$


在处理较为复杂的数据的回归问题时，普通的线性回归算法通常会出现预测精度不够，如果模型中的特征之间有相关关系，就会增加模型的复杂程度，并且对整个模型的解释能力并没有提高，这时，就需要对数据中的特征进行选择。
## 岭回归（Ridge Redression）
在平方误差的基础上加上正则项
$$
J(θ)=\frac{1}{m}\sum_{i=1}^{m}(y^{(i)}−(wx^{(i)}+b))^2+λ||w||_2^2=MSE(θ)+λ\sum_{i=1}^{n}θ_i^2
$$
通常也写成如下形式：
$$
J(θ)=\frac{1}{2m}\sum_{i=1}^{m}(y^{(i)}−(wx^{(i)}+b))^2+\frac{λ}{2}||w||_2^2=\frac{1}{2}MSE(θ)+\frac{λ}{2}\sum_{i=1}^{n}θ_i^2
$$
上式中的w是长度为n的向量，不包括截距项的系数$θ_0$；θ是长度为n+1的向量，包括截距项的系数$θ_0$；m为样本数；n为特征数.

全局最优解为：
$$
θ=(\mathbf{X}^\mathrm{T}X+λI)^{−1}(\mathbf{X}^\mathrm{T}y)
$$

## Lasso回归

加入L1正则项

代价函数为：
$$
J(θ)=\frac{1}{2m}\sum_{i=1}^{m}(y^{(i)}−(wx^{(i)}+b))^2+λ||w||_1=\frac{1}{2}MSE(θ)+λ\sum_{i=1}^{n}θ_i
$$
上式中的w是长度为n的向量，不包括截距项的系数$θ_0$, θθ是长度为n+1的向量，包括截距项的系数$θ_0$，m为样本数，n为特征数.