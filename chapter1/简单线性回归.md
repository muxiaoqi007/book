---
title: 线性回归
date: 2019-03-12 00:32:26
tags: 机器学习
---
线性回归本质上是一个函数估计的问题，即找出自变量与因变量的关系。
 - 因变量是连续的，回归
 - 因变量是离散的，分类
  回归分析是一个有监督的学习
  <!--more-->
## 简单线性回归
$$假设某个体x有d个特征，即x=(x^1,x^2,...,x^d),x^i$$是第i个特征，线性模型(linear model)试图通过特征的线性组合得到预测值，即
$$
f(x)=w^{T}x+b=w_{1}x^{1}+w_{2}x^{2}+...+w_{d}x^{n}+b
$$
<!--more-->
 其中当w_{i}是第个特征的权重，既能调节特征的量纲，也能显示该特征对预测值的重要程度；是第i个特征的权重，既能调节特征的量纲，也能显示该特征对预测值的重要程度
$$
w^{T}=（w_{1}，w_{2}，...，w_{d}）
$$

$$
x^{T}=（x_{1}，x_{2}，...，x_{d}）
$$

b代表预测值中非代表预测值中非x所能影响的那部分；当所能影响的那部分；当d=1时，便是最简单的线性模型时，便是最简单的线性模型
$$
f(x)=wx+b
$$


线性回归试图学习f(x_i)=wx_i+b，使得f(x_i)≈y_i
$$
g(w,b)=\sum_{i=1}^{n}(f(x_{i})-y_{i})^{2}=\sum_{i=1}^{n}(wx_{i}+b-y_{i})^{2}
$$
让g(w,b)取得最小值。因此我们可以用偏导数求解： 
$$
\frac{\partial g(w,b)}{\partial w}=0\\
\frac{\partial g(w,b)}{\partial b}=0
$$


&emsp;&emsp;解出：

$$
w=\frac{\sum_{i=1}^{n}y_{i}(x_{i}-\bar{x})}{\sum_{i=1}^{n}x_{i}^2-n\bar{x}^{2}}\\
b=\bar{y}-w\bar{x}\\
\bar{x}=\frac{1}{n}\sum_{i=1}^{n}x_{i}\\
\bar{y}=\frac{1}{n}\sum_{i=1}^{n}y_{i}
$$
推导过程如下：

- 先求b:
  $$
  \frac{\partial g(w,b)}{\partial b}=\sum_{i=1}^{n}(y_{i}-wx_i-b)(-1)
  =\sum_{i=1}^{n}(y_{i}-wx_i-b)=0
  $$
  化简得：
  $$
  \sum_{i=1}^{n}y_i-w\sum_{i=1}^{n}x_i-\sum_{i=1}^{n}b=\sum_{i=1}^{n}y_i-w\sum_{i=1}^{n}x_i-mb=0
  $$
  即：
  $$
  mb=\sum_{i=1}^{n}y_i-w\sum_{i=1}^{n}x_i
  $$
  即：
  $$
  b=\bar{y}-w\bar{x}
  $$

  - 再求w
    $$
    \frac{\partial g(w,b)}{\partial b}=\sum_{i=1}^{n}2(y_{i}-wx_i-b)(-x_i)=\sum_{i=1}^{n}(y_{i}-wx_i-b)x_i=0
    $$
    代入b：
    $$
    \frac{\partial g(w,b)}{\partial b}=\sum_{i=1}^{n}(y_i-wx_i-\bar{y}+w\bar{x})x_i=0
    $$
    即：
    $$
    \sum_{i=1}^{n}(x_iy_i-wx_i^2-x_i\bar{y}+wx_i\bar{x})=0
    $$
    即：
    $$
    \sum_{i=1}^{n}(x_iy_i-x_i\bar{y})=\sum_{i=1}^{n}(wx_i^2-wx_i\bar{x})
    $$
    即：
    $$
    w=\frac{\sum_{i=1}^{n}(x_iy_i-x_i\bar{y})}{\sum_{i=1}^{n}(x_i^2-x_i\bar{x})}
    $$
    w的解这样看起来很复杂，可以进一步优化

    其中：
    $$
    \sum_{i=1}^{n}x_i\bar{y}=\bar{y}\sum_{i=1}^{n}x_i=n\bar{y}\bar{x}=\bar{x}\sum_{i=1}^{n}\bar{y}=\sum_{i=1}^{n}\bar{x}y_i=\sum_{i=1}^{n}\bar{x}\bar{y}
    $$





             所以：
$$
w=\frac{\sum_{i=1}^{n}(x_iy_i-x_i\bar{y}-\bar{x}y_i+\bar{x}\bar{y})}{\sum_{i=1}^{n}(x_i^2-x_i\bar{x}-\bar{x}x_i+\bar{x})}=\frac{\sum_{i=1}^{n}(x_i-\bar{x})(y_i-\bar{y})}{\sum_{i=1}^{n}(x_i-\bar{x})^2}
$$



## 实现简单线性回归

```python
import numpy as np
import matplotlib.pyplot as plt
x = np.array([1.,2.,3.,4.,5.])
y = np.array([2.,3.,4.,3.,5.])
x_mean = np.mean(x)
y_mean = np.mean(y)
n = d = 0.0
for x_i, y_i in zip(x, y):
	n += (x_i-x_mean)*(y_i-y_mean)
	d += (x_i-x_mean)**2
w = n/d
b = y_mean - w*x_mean
y_hat = a*x + b
```

### 线性回归的评价指标

均方误差 MSE：$$\frac{1}{n}\sum_{i=1}^{n}(y_i-\hat{y_i})^2$$

均方根误差 RMSE $$\sqrt{\frac{1}{n}\sum_{i=1}^{n}(y_i-\hat{y_i})^2}$$

平均绝对误差 MAE:  $$\frac{1}{n}\sum_{i=1}^{n}|y_i-\hat{y_i}|$$

R-squared : $$1 - \frac{MSE(\hat{y}, y)}{Var(y)}$$




